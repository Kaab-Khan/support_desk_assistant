# ğŸ”§ OpenAI Client Refactoring

## ğŸ“‹ Overview

Refactored the codebase to **centralize all OpenAI API calls** into a dedicated client, following **Clean Architecture** principles.

---

## ğŸ¯ Problem Before Refactoring

### **âŒ Before: Direct OpenAI Calls Everywhere**

```
app/core/services/rag_service.py
â”œâ”€â”€ from openai import OpenAI
â”œâ”€â”€ self._openai_client = OpenAI(api_key=...)
â””â”€â”€ response = self._openai_client.chat.completions.create(...)

app/core/services/summariser_service.py
â”œâ”€â”€ from openai import OpenAI
â”œâ”€â”€ self._openai_client = OpenAI(api_key=...)
â””â”€â”€ response = self._openai_client.chat.completions.create(...)

app/infrastructure/vectorstores/pinecone_client.py
â”œâ”€â”€ from openai import OpenAI
â”œâ”€â”€ self._openai_client = OpenAI(api_key=...)
â””â”€â”€ response = self._openai_client.embeddings.create(...)
```

**Issues:**
- âŒ **Duplicated code** - Same OpenAI initialization in 3 places
- âŒ **Tight coupling** - Services directly depend on OpenAI SDK
- âŒ **Hard to test** - Must mock OpenAI in every service test
- âŒ **Hard to swap** - Can't easily replace OpenAI with another provider
- âŒ **Violates SRP** - Services handle both business logic AND API calls

---

## âœ… Solution After Refactoring

### **âœ… After: Centralized OpenAI Client**

```
app/infrastructure/clients/openai_client.py
â””â”€â”€ OpenAIClient
    â”œâ”€â”€ generate_chat_completion()
    â”œâ”€â”€ generate_embeddings()
    â”œâ”€â”€ generate_rag_response()
    â””â”€â”€ generate_summary_with_tags()

app/core/services/
â”œâ”€â”€ rag_service.py           â†’ Uses openai_client
â”œâ”€â”€ summariser_service.py    â†’ Uses openai_client
â””â”€â”€ (no direct OpenAI imports)

app/infrastructure/vectorstores/
â””â”€â”€ pinecone_client.py       â†’ Uses openai_client
```

---

## ğŸ“ New File Structure

```
app/
â”œâ”€â”€ core/
â”‚   â””â”€â”€ services/
â”‚       â”œâ”€â”€ rag_service.py              â† Uses OpenAIClient (injected)
â”‚       â””â”€â”€ summariser_service.py       â† Uses OpenAIClient (injected)
â”‚
â””â”€â”€ infrastructure/
    â”œâ”€â”€ clients/
    â”‚   â””â”€â”€ openai_client.py            â† âœ¨ NEW: Centralized OpenAI logic
    â””â”€â”€ vectorstores/
        â””â”€â”€ pinecone_client.py          â† Uses OpenAIClient (injected)
```

---

## ğŸ”§ OpenAI Client Details

### **File:** `app/infrastructure/clients/openai_client.py`

**Class:** `OpenAIClient`

**Methods:**

1. **`generate_chat_completion()`**
   - General-purpose chat completion
   - Used by any service needing LLM responses

2. **`generate_embeddings()`**
   - Generate text embeddings
   - Used by Pinecone client for document indexing

3. **`generate_rag_response()`**
   - RAG-specific response generation
   - Includes RAG prompt template
   - Used by RAG service

4. **`generate_summary_with_tags()`**
   - Summarization with tag extraction
   - Includes summarization prompt template
   - Used by Summariser service

---

## ğŸ”„ Changes Made

### **1. Created OpenAI Client**
```python
# app/infrastructure/clients/openai_client.py
class OpenAIClient:
    def __init__(self, api_key: str | None = None, model_name: str = "gpt-4o-mini"):
        settings = get_settings()
        self._api_key = api_key or settings.OPENAI_API_KEY
        self._client = OpenAI(api_key=self._api_key)
        self._model_name = model_name
    
    def generate_chat_completion(self, messages, model=None, temperature=0.7):
        """General chat completion."""
        response = self._client.chat.completions.create(...)
        return response.choices[0].message.content
    
    def generate_embeddings(self, texts, model="text-embedding-ada-002"):
        """Generate embeddings."""
        response = self._client.embeddings.create(...)
        return [item.embedding for item in response.data]
    
    # ... more methods
```

---

### **2. Refactored RAG Service**

**Before:**
```python
# app/core/services/rag_service.py
from openai import OpenAI

class RagService:
    def __init__(self, vectorstore_client, model_name=None):
        settings = get_settings()
        self._openai_client = OpenAI(api_key=settings.OPENAI_API_KEY)  # Direct!
        self._model_name = model_name or "gpt-4o-mini"
    
    def answer(self, query, top_k=5):
        # ... retrieve documents
        response = self._openai_client.chat.completions.create(  # Direct call!
            model=self._model_name,
            messages=[...]
        )
        return {"answer": response.choices[0].message.content, "sources": sources}
```

**After:**
```python
# app/core/services/rag_service.py
from app.infrastructure.clients.openai_client import OpenAIClient, get_openai_client

class RagService:
    def __init__(self, vectorstore_client, openai_client: OpenAIClient | None = None):
        self._vectorstore = vectorstore_client
        self._openai_client = openai_client or get_openai_client()  # Injected!
    
    def answer(self, query, top_k=5):
        # ... retrieve documents
        answer = self._openai_client.generate_rag_response(  # Use client method!
            query=query,
            context_chunks=context_chunks
        )
        return {"answer": answer, "sources": sources}
```

---

### **3. Refactored Summariser Service**

**Before:**
```python
# app/core/services/summariser_service.py
from openai import OpenAI

class SummariserService:
    def __init__(self, model_name=None):
        settings = get_settings()
        self._openai_client = OpenAI(api_key=settings.OPENAI_API_KEY)  # Direct!
        self._model_name = model_name or "gpt-4o-mini"
    
    def summarise(self, text, max_sentences=3):
        response = self._openai_client.chat.completions.create(  # Direct call!
            model=self._model_name,
            messages=[...]
        )
        content = response.choices[0].message.content
        # ... parse JSON
```

**After:**
```python
# app/core/services/summariser_service.py
from app.infrastructure.clients.openai_client import OpenAIClient, get_openai_client

class SummariserService:
    def __init__(self, openai_client: OpenAIClient | None = None):
        self._openai_client = openai_client or get_openai_client()  # Injected!
    
    def summarise(self, text, max_sentences=3):
        content = self._openai_client.generate_summary_with_tags(  # Use client method!
            text=text,
            max_sentences=max_sentences
        )
        # ... parse JSON
```

---

### **4. Refactored Pinecone Client**

**Before:**
```python
# app/infrastructure/vectorstores/pinecone_client.py
from openai import OpenAI

class VectorStoreClient:
    def __init__(self):
        settings = get_settings()
        self._openai_client = OpenAI(api_key=settings.OPENAI_API_KEY)  # Direct!
        self._embedding_model = "text-embedding-ada-002"
    
    def _embed_texts(self, texts):
        response = self._openai_client.embeddings.create(  # Direct call!
            model=self._embedding_model,
            input=texts
        )
        return [item.embedding for item in response.data]
```

**After:**
```python
# app/infrastructure/vectorstores/pinecone_client.py
from app.infrastructure.clients.openai_client import OpenAIClient, get_openai_client

class VectorStoreClient:
    def __init__(self, openai_client: OpenAIClient | None = None):
        self._pc = Pinecone(api_key=settings.PINECONE_API_KEY)
        self._openai_client = openai_client or get_openai_client()  # Injected!
    
    def _embed_texts(self, texts):
        return self._openai_client.generate_embeddings(texts)  # Use client method!
```

---

## âœ… Benefits of This Refactoring

### **1. Single Responsibility Principle (SRP)**
- Services focus on **business logic** only
- OpenAI client handles **API communication** only

### **2. Dependency Injection**
```python
# Easy to inject mock for testing
mock_openai = Mock(spec=OpenAIClient)
rag_service = RagService(vectorstore, openai_client=mock_openai)
```

### **3. Testability**
```python
# Before: Must mock OpenAI SDK directly
@patch('app.core.services.rag_service.OpenAI')
def test_rag_service(mock_openai):
    # Complex setup...

# After: Mock the client interface
mock_client = Mock()
mock_client.generate_rag_response.return_value = "Answer"
service = RagService(vectorstore, openai_client=mock_client)
# Simple and clean!
```

### **4. Flexibility to Swap Providers**
```python
# Easy to create alternative implementations
class AnthropicClient:
    def generate_rag_response(self, query, context_chunks):
        # Use Anthropic's Claude instead
        pass

# Use it without changing services
rag_service = RagService(vectorstore, openai_client=anthropic_client)
```

### **5. Centralized Prompt Management**
- All prompts in one place (`openai_client.py`)
- Easy to update prompt templates
- Consistent prompt engineering

### **6. Reduced Code Duplication**
- OpenAI initialization: **1 place** (was 3)
- Chat completion logic: **1 place** (was 2)
- Embedding logic: **1 place** (was 1, but cleaner)

### **7. Better Error Handling**
```python
# Can add retry logic, rate limiting, etc. in one place
class OpenAIClient:
    def generate_chat_completion(self, ...):
        try:
            response = self._client.chat.completions.create(...)
            return response.choices[0].message.content
        except OpenAIError as e:
            # Centralized error handling
            logger.error(f"OpenAI API error: {e}")
            raise ServiceException("LLM service unavailable")
```

---

## ğŸ“Š Comparison

| Aspect | Before | After |
|--------|--------|-------|
| **OpenAI imports** | 3 files | 1 file âœ… |
| **Initialization code** | Duplicated 3x | Once âœ… |
| **Testing complexity** | Mock SDK in each test | Mock client interface âœ… |
| **Provider switching** | Change 3 files | Change 1 file âœ… |
| **Prompt updates** | Scattered in services | Centralized âœ… |
| **Error handling** | Duplicated | Centralized âœ… |
| **Code coupling** | Tight (SDK) | Loose (interface) âœ… |

---

## ğŸ§ª Testing Strategy

### **Unit Test Example:**
```python
# tests/unit/services/test_rag_service.py
from unittest.mock import Mock

def test_rag_service_answer():
    # Mock dependencies
    mock_vectorstore = Mock()
    mock_vectorstore.query_similar.return_value = [...]
    
    mock_openai = Mock()
    mock_openai.generate_rag_response.return_value = "Test answer"
    
    # Create service with mocks
    service = RagService(
        vectorstore_client=mock_vectorstore,
        openai_client=mock_openai
    )
    
    # Test
    result = service.answer("Test query")
    
    # Verify
    assert result["answer"] == "Test answer"
    mock_openai.generate_rag_response.assert_called_once()
```

---

## ğŸ“ Design Patterns Used

### **1. Dependency Injection**
- Services receive OpenAI client via constructor
- Loose coupling, easy testing

### **2. Singleton Pattern**
```python
_openai_client: OpenAIClient | None = None

def get_openai_client() -> OpenAIClient:
    global _openai_client
    if _openai_client is None:
        _openai_client = OpenAIClient()
    return _openai_client
```

### **3. Adapter Pattern**
- OpenAIClient adapts OpenAI SDK to our interface
- Can easily swap implementations

### **4. Facade Pattern**
- OpenAIClient provides simple interface to complex OpenAI API
- Hides SDK complexity from services

---

## ğŸš€ Future Enhancements

### **1. Add More Providers**
```python
# app/infrastructure/clients/anthropic_client.py
class AnthropicClient:
    def generate_rag_response(self, query, context_chunks):
        # Use Claude API
        pass

# app/infrastructure/clients/llm_factory.py
def get_llm_client(provider="openai"):
    if provider == "openai":
        return get_openai_client()
    elif provider == "anthropic":
        return get_anthropic_client()
```

### **2. Add Caching Layer**
```python
class CachedOpenAIClient(OpenAIClient):
    def generate_embeddings(self, texts):
        # Check cache first
        cached = self._cache.get(texts)
        if cached:
            return cached
        
        # Otherwise call API
        embeddings = super().generate_embeddings(texts)
        self._cache.set(texts, embeddings)
        return embeddings
```

### **3. Add Rate Limiting**
```python
class RateLimitedOpenAIClient(OpenAIClient):
    def generate_chat_completion(self, ...):
        self._rate_limiter.wait()
        return super().generate_chat_completion(...)
```

---

## ğŸ“ Summary

**Before:** Services directly called OpenAI SDK (tight coupling)  
**After:** Services use OpenAI Client (loose coupling)

**Benefits:**
- âœ… Cleaner code
- âœ… Easier testing
- âœ… Better maintainability
- âœ… Flexibility to swap providers
- âœ… Centralized API logic
- âœ… Single Responsibility Principle

**Architecture Pattern:** Clean Architecture with Dependency Injection

**Time to refactor:** ~30 minutes  
**Impact:** High - Improved code quality significantly! ğŸš€
